# Distancia de Hamming y An√°lisis de Lenguajes

**Ruta:** [üìö Fundamentos de Electr√≥nica](../../../index.md) > [M√≥dulo 6: Fundamentos de Electr√≥nica Digital](../../index.md) > [Sistemas de Representaci√≥n de la Informaci√≥n](../index.md) > [Alfabetos, Lenguajes y Sem√°ntica](index.md)
[‚¨ÖÔ∏è Anterior](propiedades_de_codigos_adyacente_ciclico_saturado.md)

---

**ID:** `1.6.1.1.3`

## üìù Contenido Te√≥rico

### 1. Introducci√≥n

La **distancia de Hamming** es una m√©trica fundamental en teor√≠a de la informaci√≥n y c√≥digos detectores/correctores de errores. Fue introducida por Richard Hamming en 1950 como parte de su trabajo sobre detecci√≥n y correcci√≥n de errores en sistemas de comunicaci√≥n digital.

#### Definici√≥n Formal

> **Definici√≥n**: Sea Œ£ un alfabeto finito. La distancia de Hamming entre dos palabras x, y ‚àà Œ£‚Åø de igual longitud n es:
>
> ```
> d_H(x, y) = |{i : 1 ‚â§ i ‚â§ n, x_i ‚â† y_i}|
> ```
>
> Es decir, el n√∫mero de posiciones en las que x e y difieren.

#### Ejemplo Introductorio

Para cadenas binarias:

```
    Posici√≥n (columnas):
    0  1  2  3  4  5  6
x = 1  0  1  1  0  1  0
y = 1  0  0  1  1  1  0
    ‚úì ‚úì  ‚úó  ‚úì ‚úó  ‚úì  ‚úì d_H(x,y) = 2
    Posici√≥n (columnas):
    0  1  2  3  4  5  6
w = 1  0  2  9  8  7  0
z = 1  0  0  9  3  7  0
    ‚úì ‚úì  ‚úó  ‚úì ‚úó  ‚úì  ‚úì d_H(w,z) = 2
```

d_H(x, y) = 2 (difieren en las posiciones 2 y 4)

### 2. Propiedades Matem√°ticas

#### 2.1 La Distancia de Hamming es una Distancia M√©trica

**Lema 1**: Aditividad de la distancia de Hamming en subpalabras de ancho fijho de $n$ d√≠gitos, con subdivisiones de longitud $n-1$ d√≠gitos y $1$ d√≠gitos.

```Lemma
Sea $\Sigma$ un alfabeto finito.
Sea $k \in \SetNat$
Sea ${\Sigma}^k$ el conjunto de todas las palabras de longitud $k$ sobre el alfabeto $\Sigma$.
$‚àÄ x y ‚àà {\Sigma}^k$, sean $x = x_{0} ¬∑ x_{[1:k-1]}$ y $y = y_{0} ¬∑ y_{[1:k-1]}$ se cumple que:
    $d_H(x, y) = d_H(x_{0}, y_{0}) + d_H(x_{[1:k-1]}, y_{[1:k-1]})$
```

**Demostraci√≥n**:

```Proof
Sean $x ‚àà {\Sigma}^k$ y $y ‚àà {\Sigma}^k$ dos palabras de longitud $k$.
Caso base: $k = 0$
    Si $k = 0$, entonces $x = Œµ$ y $y = Œµ$ (palabra vac√≠a).
    Luego:
       $d_H(x, y) = d_H(Œµ, Œµ) = 0$
       $d_H(x_{0}, y_{0}) + d_H(x_{[1:k-1]}, y_{[1:k-1]}) = d_H(Œµ, Œµ) + d_H(Œµ, Œµ) 
                                                          = 0 + 0 
                                                          = 0$ ‚úì
    
    Por tanto, el caso base $k=0$ se cumple.
Caso base: $k = 1$
    Si $k = 1$, entonces $x = x_{0}$ y $y = y_{0}$.
    Luego:
        $d_H(x, y) = d_H(x_{0}, y_{0}) + d_H(x_{[1:0]}, y_{[1:0]})$ 
                  $= d_H(x_{0}, y_{0}) + d_H(Œµ, Œµ)$ 
                  $= d_H(x_{0}, y_{0}) + 0$ 
                  $= d_H(x, y)$  ‚úì
    
    Por tanto, el caso base $k=1$ se cumple.
Hip√≥tesis de inducci√≥n:
    Supongamos que para $k = t > 1$ se cumple que:
        $‚àÄ x y ‚àà {\Sigma}^t$, sean $x = x[0] ¬∑ x[1:t-1]$ y $y = y[0] ¬∑ y[1:t-1]$ se cumple que:
            $d_H(x, y) = d_H(x[0], y[0]) + d_H(x[1:t-1], y[1:t-1])$
Paso inductivo:
    Debemos demostrar que para $k = t + 1$ se cumple la propiedad.
    
    Sean $x ‚àà {\Sigma}^{t+1}$ y $y ‚àà {\Sigma}^{t+1}$ dos palabras de longitud $t + 1$.
    Entonces $x = x[0] ¬∑ x[1:t]$ y $y = y[0] ¬∑ y[1:t]$.
    
    Por definici√≥n de distancia de Hamming sobre palabras concatenadas:
    Las posiciones de x e y son:
      - Posici√≥n 0: $x[0]$ vs $y[0]$
      - Posiciones 1 a t: $x[1:t]$ vs $y[1:t]$
    
    Luego:
        $d_H(x, y) = d_H(x[0] ¬∑ x[1:t], y[0] ¬∑ y[1:t])$ # Definici√≥n de la concatenaci√≥n
                  $= |{i : 0 ‚â§ i ‚â§ t, (x[0]¬∑x[1:t])·µ¢ ‚â† (y[0]¬∑y[1:t])·µ¢}|$ # Definici√≥n de d_H
    
    Particionamos el conjunto de √≠ndices {0, 1, ..., t} = {0} ‚à™ {1, 2, ..., t} (disjuntos).
    Por propiedades de indexaci√≥n de concatenaci√≥n:
      - (x[0]¬∑x[1:t])‚ÇÄ = x[0]  y  (y[0]¬∑y[1:t])‚ÇÄ = y[0]
      - Para i ‚â• 1: (x[0]¬∑x[1:t])·µ¢ = x[1:t]·µ¢‚Çã‚ÇÅ  y  (y[0]¬∑y[1:t])·µ¢ = y[1:t]·µ¢‚Çã‚ÇÅ
    
    Por tanto:
                  $= |{0 : x[0] ‚â† y[0]}| + |{i : 1 ‚â§ i ‚â§ t, x[1:t]·µ¢‚Çã‚ÇÅ ‚â† y[1:t]·µ¢‚Çã‚ÇÅ}|$ # Partici√≥n disjunta
    
    Aplicamos la propiedad fundamental: $|A ‚à™ B| = |A| + |B| + |A ‚à© B|$ para conjuntos disjuntos.
    Observamos que:
      - $|{0 : x[0] ‚â† y[0]}| = d_H(x[0], y[0])$  [$x[0]$, $y[0]$ son s√≠mbolos √∫nicos]
      - $|{i : 1 ‚â§ i ‚â§ t, x[1:t]·µ¢‚Çã‚ÇÅ ‚â† y[1:t]·µ¢‚Çã‚ÇÅ}| = d_H(x[1:t], y[1:t])$  [por def. de d_H sobre palabras]
    
    Por tanto:
                  $= d_H(x[0], y[0]) + d_H(x[1:t], y[1:t])$  ‚úì
    
    **Observaci√≥n**: Este resultado muestra que la concatenaci√≥n de palabras es un homomorfismo 
    respecto a la descomposici√≥n aditiva de la distancia de Hamming:
        d_H(u¬∑v, u'¬∑v') = d_H(u, u') + d_H(v, v')  cuando |u| = |u'| y |v| = |v'|
    
    Por tanto, el paso inductivo se cumple.
```

**Teorema 1**: La distancia Hamming de las subpalabras de ancho fijo menor que $n$ es estrictamente aditiva.

```Theorem
${\Sigma}^k$ es el conjunto de todas las palabras de longitud $k ‚àà \SetNat$ sobre el alfabeto $\Sigma$.
Sean $n, m ‚àà \SetNat$ con $n+m=k$
Sean $x, y ‚àà {\Sigma}^k$ dos palabras de longitud $k$.
Sean $x = x[0:n-1] ¬∑ x[n:k-1]$ y $y = y[0:n-1] ¬∑ y[n:k-1]$
Entonces: d_H(x, y) = d_H(x[0:n-1], y[0:n-1]) + d_H(x[n:k-1], y[n:k-1])
```

**Demostraci√≥n**:

```Proof
Por el Lema 1, sabemos que para cualquier palabra w de longitud k:
    d_H(w[0]¬∑w[1:k-1], z[0]¬∑z[1:k-1]) = d_H(w[0], z[0]) + d_H(w[1:k-1], z[1:k-1])

Demostraremos el teorema por inducci√≥n sobre n (longitud del prefijo).

Caso base (n = 0):
    Si n = 0, entonces x[0:n-1] = Œµ (palabra vac√≠a) y x[n:k-1] = x (palabra completa).
    Luego:
        d_H(x, y) = d_H(Œµ¬∑x, Œµ¬∑y) = d_H(Œµ, Œµ) + d_H(x, y) = 0 + d_H(x, y)  ‚úì

Caso base (n = 1):
    Si n = 1, entonces x = x[0]¬∑x[1:k-1] y y = y[0]¬∑y[1:k-1].
    Por Lema 1:
        d_H(x, y) = d_H(x[0], y[0]) + d_H(x[1:k-1], y[1:k-1])  ‚úì

Hip√≥tesis de inducci√≥n:
    Supongamos que para n = t se cumple:
        d_H(x, y) = d_H(x[0:t-1], y[0:t-1]) + d_H(x[t:k-1], y[t:k-1])

Paso inductivo (n = t + 1):
    Sean x, y palabras de longitud k, con descomposici√≥n x[0:t]¬∑x[t+1:k-1].
    
    Aplicamos Lema 1 con prefijo de longitud 1:
        d_H(x, y) = d_H(x[0]¬∑x[1:k-1], y[0]¬∑y[1:k-1])
                  = d_H(x[0], y[0]) + d_H(x[1:k-1], y[1:k-1])
    
    Ahora, sobre x[1:k-1] (que tiene longitud k-1), aplicamos la HI con n' = t:
        d_H(x[1:k-1], y[1:k-1]) = d_H(x[1:t], y[1:t]) + d_H(x[t+1:k-1], y[t+1:k-1])
    
    Sustituyendo:
        d_H(x, y) = d_H(x[0], y[0]) + d_H(x[1:t], y[1:t]) + d_H(x[t+1:k-1], y[t+1:k-1])
                  = d_H(x[0:t], y[0:t]) + d_H(x[t+1:k-1], y[t+1:k-1])  ‚úì
    
    Por tanto, el resultado se cumple para n = t + 1.

Por inducci√≥n matem√°tica, el teorema se cumple para todo n ‚àà ‚Ñï con 0 ‚â§ n ‚â§ k.
```

**Observaci√≥n**: Este teorema muestra que la distancia de Hamming se comporta aditivamente
sobre cualquier partici√≥n de las palabras en subpalabras contiguas. Es decir, para cualquier
descomposici√≥n x = x‚ÇÅ¬∑x‚ÇÇ¬∑...¬∑x‚Çò con |x·µ¢| = n·µ¢ y Œ£n·µ¢ = k:
    d_H(x, y) = Œ£·µ¢ d_H(x·µ¢, y·µ¢)

Esta propiedad es fundamental para el an√°lisis de c√≥digos de bloque.

#### 2.2 La Distancia de Hamming es una M√©trica

**Teorema 2**: La distancia de Hamming d_H define una m√©trica sobre Œ£‚Åø.

Para demostrar que d_H es una m√©trica, debemos probar que satisface las tres propiedades de una m√©trica:

##### Propiedad M1: No Negatividad e Identidad

```

‚àÄx, y ‚àà Œ£‚Åø: d_H(x, y) ‚â• 0
‚àÄx, y ‚àà Œ£‚Åø: d_H(x, y) = 0 ‚ü∫ x = y

```

**Demostraci√≥n**:

- **No negatividad**: d_H(x, y) cuenta elementos, por tanto d_H(x, y) ‚â• 0
- **Identidad**:
  - (‚üπ) Si x = y, entonces ‚àÄi: x_i = y_i, luego no hay posiciones diferentes, d_H(x, y) = 0
  - (‚ü∏) Si d_H(x, y) = 0, no hay posiciones diferentes, luego ‚àÄi: x_i = y_i, por tanto x = y
- **Positividad**: Por inducci√≥n, si $x, y ‚àà Œ£^1$, y $x \neq y$ entonces $x_0 ‚â† y_0$. $d_H(x, y) = 1$
  - Hip√≥tesis de inducci√≥n: Supongamos que para $n=k$ se cumple que si $x, y ‚àà Œ£^k$ y $x \neq y$ entonces $d_H(x, y) ‚â• 1$
  - Paso inductivo: Para $n=k+1$, sean $x, y ‚àà Œ£^{k+1}$ y $x \neq y$. Entonces $d_H(x, y) = d_H(x[0:k-1], y[0:k-1]) + d_H(x[k],y[k])$. Hay 3 casos posibles:
    - $x[0:k-1] = y[0:k-1]$, entonces $x[k] \neq y[k]$ y por el caso base $d_H(x[0:k], y[0:k]) = d_H(x[0:k-1],y[0:k-1]) + d_H(x[k],y[k]) = 0 + 1 = 1 \le 1$, luego $d_H(x, y) = d_H(x[0:k], y[0:k]) ‚â• 1$.
    - $x[0:k-1] \neq y[0:k-1]$, entonces por hip√≥tesis de inducci√≥n $d_H(x[0:k-1], y[0:k-1]) ‚â• 1$ y $d_H(x[k],y[k]) \le 0$, luego $d_H(x, y) = d_H(x[0:k], y[0:k]) = d_H(x[0:k-1], y[0:k-1]) + d_H(x[k],y[k]) ‚â• 1 + 0 = 1$.
    -

‚ñ°

##### Propiedad M2: Simetr√≠a

```

‚àÄx, y ‚àà Œ£‚Åø: d_H(x, y) = d_H(y, x)

```

**Demostraci√≥n**:

```

d_H(x, y) = |{i : x_i ‚â† y_i}|
          = |{i : y_i ‚â† x_i}|    (la desigualdad es sim√©trica)
          = d_H(y, x)

```

‚ñ°

##### Propiedad M3: Desigualdad Triangular

```

‚àÄx, y, z ‚àà Œ£‚Åø: d_H(x, z) ‚â§ d_H(x, y) + d_H(y, z)

```

**Demostraci√≥n**:

Sea I_{xy} = {i : x_i ‚â† y_i}, I_{yz} = {i : y_i ‚â† z_i}, I_{xz} = {i : x_i ‚â† z_i}

Para cada posici√≥n i hay tres casos:

1. **x_i = y_i = z_i**: i ‚àâ I_{xy} ‚à™ I_{yz} ‚à™ I_{xz}
2. **Dos son iguales, uno diferente**:
   - Si x_i = y_i ‚â† z_i: entonces i ‚àà I_{yz} ‚à© I_{xz}, pero i ‚àâ I_{xy}
   - Si x_i = z_i ‚â† y_i: entonces i ‚àà I_{xy} ‚à© I_{yz}, pero i ‚àâ I_{xz}
   - Si y_i = z_i ‚â† x_i: entonces i ‚àà I_{xy} ‚à© I_{xz}, pero i ‚àâ I_{yz}
3. **Todos diferentes**: x_i ‚â† y_i ‚â† z_i ‚â† x_i: entonces i ‚àà I_{xy} ‚à© I_{yz}

En todos los casos: Si i ‚àà I_{xz}, entonces i ‚àà I_{xy} ‚à™ I_{yz}

Por tanto: I_{xz} ‚äÜ I_{xy} ‚à™ I_{yz}

Luego: |I_{xz}| ‚â§ |I_{xy} ‚à™ I_{yz}| ‚â§ |I_{xy}| + |I_{yz}|

Es decir: d_H(x, z) ‚â§ d_H(x, y) + d_H(y, z)
‚ñ°

**Conclusi√≥n**: Hemos demostrado que la distancia de Hamming es una m√©trica formal. Esta demostraci√≥n est√° implementada como prueba formal en el sistema de l√≥gica matem√°tica del proyecto (ver secci√≥n de demostraciones formales).

#### 2.3 Peso de Hamming

**Definici√≥n**: El **peso de Hamming** de una palabra x ‚àà Œ£‚Åø, denotado w_H(x), es el n√∫mero de posiciones no nulas (diferentes del s√≠mbolo cero del alfabeto):

```
w_H(x) = |{i : x_i ‚â† 0}|
```

Para alfabetos binarios Œ£ = {0, 1}: w_H(x) = n√∫mero de unos en x

**Ejemplos**:

- w_H(0000) = 0
- w_H(1010) = 2
- w_H(1111) = 4
- w_H(10110101) = 5

**Proposici√≥n 1**: El peso de Hamming es la distancia al origen

```
‚àÄx ‚àà Œ£‚Åø: w_H(x) = d_H(x, 0‚Åø)
```

**Demostraci√≥n**:

```Proof
Sea 0‚Åø = 00...0 (n ceros) la palabra nula.

Por definici√≥n de distancia de Hamming:
    d_H(x, 0‚Åø) = |{i : x_i ‚â† 0}|

Por definici√≥n de peso de Hamming:
    w_H(x) = |{i : x_i ‚â† 0}|

Por tanto: w_H(x) = d_H(x, 0‚Åø)  ‚úì
```

**Teorema 3** (Relaci√≥n peso-distancia en alfabetos con estructura de grupo):

Para alfabetos con operaci√≥n de grupo (Œ£, ‚äï), en particular para F‚ÇÇ = {0,1} con XOR:

```
‚àÄx, y ‚àà Œ£‚Åø: d_H(x, y) = w_H(x ‚äï y)
```

donde (x ‚äï y)·µ¢ = x·µ¢ ‚äï y·µ¢ (operaci√≥n componente a componente)

**Demostraci√≥n**:

```Proof
Sea z = x ‚äï y, donde z·µ¢ = x·µ¢ ‚äï y·µ¢ para cada posici√≥n i.

Por definici√≥n de distancia de Hamming:
    d_H(x, y) = |{i : x_i ‚â† y_i}|

Por propiedades de la operaci√≥n XOR en F‚ÇÇ:
    x_i ‚â† y_i ‚ü∫ x_i ‚äï y_i = 1
    x_i = y_i ‚ü∫ x_i ‚äï y_i = 0

Por tanto:
    {i : x_i ‚â† y_i} = {i : (x ‚äï y)·µ¢ ‚â† 0} = {i : z_i ‚â† 0}

Luego:
    d_H(x, y) = |{i : x_i ‚â† y_i}|
              = |{i : z_i ‚â† 0}|
              = w_H(z)
              = w_H(x ‚äï y)  ‚úì
```

**Aplicaci√≥n pr√°ctica**: En circuitos digitales, d_H(x, y) se puede calcular como:

1. Aplicar XOR bit a bit: z = x ‚äï y
2. Contar los unos en z (circuito contador de poblaci√≥n/"popcount")

**Propiedades del peso de Hamming**:

1. **No negatividad**: w_H(x) ‚â• 0 para todo x
2. **Nulidad**: w_H(x) = 0 ‚ü∫ x = 0‚Åø
3. **Aditividad** (en F‚ÇÇ): w_H(x ‚äï y) ‚â§ w_H(x) + w_H(y) (desigualdad triangular trasladada)
4. **Invariancia por permutaci√≥n**: w_H(œÄ(x)) = w_H(x) para cualquier permutaci√≥n œÄ

#### 2.4 Esferas de Hamming y Volumen

**Definici√≥n**: La **esfera de Hamming** de radio r centrada en x es:

```
B(x, r) = {y ‚àà Œ£‚Åø : d_H(x, y) ‚â§ r}
```

Es el conjunto de todas las palabras a distancia ‚â§ r de x.

**Teorema 4** (Volumen de esferas de Hamming):

El n√∫mero de palabras en una esfera de radio r es:

```
V(n, r) = |B(x, r)| = Œ£·µ¢‚Çå‚ÇÄ ≥ C(n, i) ¬∑ (|Œ£| - 1)‚Å±
```

donde C(n, i) = (n choose i) = n!/(i!(n-i)!)

**Demostraci√≥n**:

```Proof
El volumen V(n, r) es independiente del centro x (por invariancia translacional de la m√©trica).
Tomemos x = 0‚Åø sin p√©rdida de generalidad.

Una palabra y est√° en B(0‚Åø, r) si y solo si w_H(y) ‚â§ r.

Para cada distancia exacta i (con 0 ‚â§ i ‚â§ r), contamos cu√°ntas palabras tienen exactamente i s√≠mbolos no nulos:

1. **Elegir posiciones**: Hay C(n, i) formas de elegir i posiciones de n
2. **Elegir s√≠mbolos no nulos**: Para cada posici√≥n elegida, hay (|Œ£| - 1) opciones 
   (cualquier s√≠mbolo excepto 0)
3. **Posiciones restantes**: Las n - i posiciones restantes deben ser 0

Por tanto, hay C(n, i) ¬∑ (|Œ£| - 1)‚Å± palabras a distancia exactamente i.

Sumando sobre todas las distancias de 0 a r:
    V(n, r) = Œ£·µ¢‚Çå‚ÇÄ ≥ C(n, i) ¬∑ (|Œ£| - 1)‚Å±  ‚úì
```

**Caso particular** (alfabeto binario Œ£ = {0, 1}):

```
V(n, r) = Œ£·µ¢‚Çå‚ÇÄ ≥ C(n, i)
```

**Ejemplos**:

Para n = 5, Œ£ = {0, 1}:

- V(5, 0) = C(5,0) = 1 (solo la palabra central)
- V(5, 1) = C(5,0) + C(5,1) = 1 + 5 = 6
- V(5, 2) = 1 + 5 + 10 = 16
- V(5, 5) = 2‚Åµ = 32 (todo el espacio)

**Teorema 5** (Hamming Bound o Sphere-Packing Bound):

Sea C ‚äÜ Œ£‚Åø un c√≥digo con distancia m√≠nima $d_{min} = 2t + 1$ (corrige hasta t errores).
Entonces:

```
|C| ‚â§ |Œ£|‚Åø / V(n, t)
```

**Demostraci√≥n**:

```Proof
Si C corrige hasta t errores, entonces las esferas B(c, t) centradas en cada palabra-c√≥digo c ‚àà C 
deben ser disjuntas (no solapadas).

**Justificaci√≥n**: Supongamos que B(c‚ÇÅ, t) ‚à© B(c‚ÇÇ, t) ‚â† ‚àÖ para c‚ÇÅ ‚â† c‚ÇÇ.
Entonces existe y tal que d_H(y, c‚ÇÅ) ‚â§ t y d_H(y, c‚ÇÇ) ‚â§ t.
Por desigualdad triangular:
    d_H(c‚ÇÅ, c‚ÇÇ) ‚â§ d_H(c‚ÇÅ, y) + d_H(y, c‚ÇÇ) ‚â§ t + t = 2t

Pero $d_{min} = 2t + 1$, contradicci√≥n. Por tanto, las esferas son disjuntas.

Como hay |C| palabras-c√≥digo y cada esfera tiene volumen V(n, t):
    |C| ¬∑ V(n, t) ‚â§ |Œ£‚Åø| = |Œ£|‚Åø

Dividiendo por V(n, t):
    |C| ‚â§ |Œ£|‚Åø / V(n, t)  ‚úì
```

**Interpretaci√≥n**: Este teorema establece un **l√≠mite superior** para el n√∫mero de palabras-c√≥digo que puede tener un c√≥digo con capacidad de correcci√≥n t. Es una restricci√≥n fundamental en teor√≠a de c√≥digos.

**Definici√≥n**: Un c√≥digo que alcanza la igualdad |C| = |Œ£|‚Åø / V(n, t) se llama **c√≥digo perfecto**, porque las esferas de radio t "empacan" completamente el espacio Œ£‚Åø sin huecos ni solapamientos.

**Ejemplos de c√≥digos perfectos**:

- C√≥digos de Hamming ($d_{min} = 3$, t = 1)
- C√≥digo de Golay binario [23, 12, 7]
- C√≥digo de repetici√≥n [n, 1, n] con n impar

#### 2.5 Distancia Promedio

**Definici√≥n**: La **distancia promedio** entre dos palabras aleatorias uniformemente distribuidas en Œ£‚Åø es:

```
E[d_H(X, Y)] = Valor esperado de d_H cuando X, Y ~ Uniforme(Œ£‚Åø)
```

**Teorema 6** (Distancia promedio):

Para X, Y palabras aleatorias independientes uniformemente distribuidas en Œ£‚Åø:

```
E[d_H(X, Y)] = n ¬∑ (|Œ£| - 1) / |Œ£| = n ¬∑ (1 - 1/|Œ£|)
```

**Demostraci√≥n**:

```Proof
Por linealidad de la esperanza y la definici√≥n de d_H:
    E[d_H(X, Y)] = E[Œ£·µ¢‚Çå‚ÇÄ‚Åø‚Åª¬π ùüô{X·µ¢ ‚â† Y·µ¢}]
                  = Œ£·µ¢‚Çå‚ÇÄ‚Åø‚Åª¬π E[ùüô{X·µ¢ ‚â† Y·µ¢}]
                  = Œ£·µ¢‚Çå‚ÇÄ‚Åø‚Åª¬π P(X·µ¢ ‚â† Y·µ¢)

Para cada posici√≥n i:
    P(X·µ¢ = Y·µ¢) = Œ£‚Çõ‚ààŒ£ P(X·µ¢ = s) ¬∑ P(Y·µ¢ = s)
                = Œ£‚Çõ‚ààŒ£ (1/|Œ£|) ¬∑ (1/|Œ£|)    [por independencia y uniformidad]
                = |Œ£| ¬∑ (1/|Œ£|¬≤)
                = 1/|Œ£|

Por tanto:
    P(X·µ¢ ‚â† Y·µ¢) = 1 - P(X·µ¢ = Y·µ¢) = 1 - 1/|Œ£| = (|Œ£| - 1)/|Œ£|

Sustituyendo:
    E[d_H(X, Y)] = Œ£·µ¢‚Çå‚ÇÄ‚Åø‚Åª¬π (|Œ£| - 1)/|Œ£|
                  = n ¬∑ (|Œ£| - 1)/|Œ£|  ‚úì
```

**Casos particulares**:

1. **Alfabeto binario** (Œ£ = {0, 1}, |Œ£| = 2):

   ```
   E[d_H(X, Y)] = n ¬∑ 1/2 = n/2
   ```

   Interpretaci√≥n: En promedio, dos palabras binarias aleatorias difieren en la mitad de sus bits.

2. **Alfabeto cuaternario** (Œ£ = {0, 1, 2, 3}, |Œ£| = 4):

   ```
   E[d_H(X, Y)] = n ¬∑ 3/4 = 3n/4
   ```

3. **Alfabeto general de tama√±o q**:

   ```
   E[d_H(X, Y)] = n(q-1)/q
   ```

**Varianza de la distancia de Hamming**:

```Proposition
Var[d_H(X, Y)] = n ¬∑ P(X·µ¢ ‚â† Y·µ¢) ¬∑ P(X·µ¢ = Y·µ¢)
                = n ¬∑ (|Œ£| - 1)/|Œ£| ¬∑ 1/|Œ£|
                = n(|Œ£| - 1)/|Œ£|¬≤
```

Para alfabeto binario: Var[d_H(X, Y)] = n/4

**Aplicaci√≥n pr√°ctica**:

La distancia promedio proporciona una **l√≠nea base** para evaluar c√≥digos:

- Si $d_{min}$ de un c√≥digo es mucho mayor que E[d_H], el c√≥digo tiene buena separaci√≥n
- Para c√≥digos binarios de longitud n, queremos $d_{min} >> n/2$ para robustez

**Ejemplo**:

- C√≥digo con n = 16, $d_{min} = 8$: Est√° en E[d_H] = 8 (apenas adecuado)
- C√≥digo con n = 16, $d_{min} = 12$: Est√° bien por encima del promedio (excelente)

### 3. Distancia M√≠nima de un Lenguaje

#### Definici√≥n

Sea L ‚äÜ Œ£‚Åø un lenguaje (conjunto de palabras de longitud n). La **distancia m√≠nima** de L es:

```

$d_{min}(L) = min\{d_H(x, y) : x, y \in L, x \neq y\}$

```

#### Importancia

La distancia m√≠nima determina la **capacidad de detecci√≥n y correcci√≥n de errores**:

| $d_{min}$ | Capacidad |
|-------|-----------|
| $d_{min} = 1$ | No detecta errores (palabras adyacentes) |
| $d_{min} = 2$ | Detecta 1 error |
| $d_{min} = 3$ | Detecta 2 errores, corrige 1 error |
| $d_{min} = 4$ | Detecta 3 errores, corrige 1 error |
| $d_{min} = 2t+1$ | Corrige hasta t errores |
**Teorema 2**: Un c√≥digo con distancia m√≠nima d puede:

- **Detectar** hasta d-1 errores
- **Corregir** hasta ‚åä(d-1)/2‚åã errores

#### Ejemplo: C√≥digo de Repetici√≥n Triple

```python
L = {000, 111}  # Alfabeto Œ£ = {0, 1}
$d_{min}(L) = d_H(000, 111) = 3$
```

Este c√≥digo puede:

- Detectar hasta 2 errores
- Corregir 1 error (por votaci√≥n mayoritaria)

Ejemplo de correcci√≥n:

```
Enviado:  111
Recibido: 101  (error en posici√≥n 2)
Decodificaci√≥n: 
  d_H(101, 000) = 2
  d_H(101, 111) = 1  ‚Üê m√°s cercano
Resultado: 111 (correcto)
```

### 4. Esferas de Hamming

#### Definici√≥n

La **esfera de Hamming** de radio r centrada en x es:

```
S_r(x) = {y ‚àà Œ£‚Åø : d_H(x, y) ‚â§ r}
```

El conjunto de todas las palabras a distancia como m√°ximo r de x.

#### Volumen de una Esfera

Para el alfabeto binario Œ£ = {0, 1}:

```
|S_r(x)| = Œ£(i=0 to r) C(n, i)
```

donde C(n, i) es el coeficiente binomial.

**Justificaci√≥n**: Hay C(n, i) formas de elegir i posiciones de n para cambiar.

Ejemplo para n=7, r=1:

```
|S_1(x)| = C(7,0) + C(7,1) = 1 + 7 = 8
```

(la palabra original + 7 palabras con 1 bit cambiado)

### 5. Cota de Hamming

**Teorema 3 (Cota de Hamming)**:

Para un c√≥digo C ‚äÜ {0,1}‚Åø con distancia m√≠nima d = 2t+1:

```
|C| ¬∑ Œ£(i=0 to t) C(n, i) ‚â§ 2‚Åø
```

**Interpretaci√≥n**: Las esferas de radio t alrededor de cada palabra c√≥digo no se solapan, y todas deben caber en el espacio {0,1}‚Åø.

**C√≥digo perfecto**: Cuando se alcanza la igualdad, el c√≥digo se llama **perfecto**. Ejemplos:

- C√≥digo de Hamming (7,4): n=7, t=1, |C|=16
- C√≥digo de Golay (23,12): n=23, t=3, |C|=4096

### 6. Aplicaciones Pr√°cticas

#### 6.1 Detecci√≥n de Errores

La distancia de Hamming se usa en:

- **C√≥digos de paridad**: $d_{min} = 2$ (detecta 1 error)
- **CRC (Cyclic Redundancy Check)**: detecta r√°fagas de errores
- **Checksums**: verificaci√≥n de integridad

#### 6.2 Correcci√≥n de Errores

C√≥digos correctores:

- **Hamming (7,4)**: 4 bits de datos + 3 de paridad, corrige 1 error
- **Reed-Solomon**: usado en CD, DVD, QR codes
- **Turbo codes**: telecomunicaciones 4G/5G
- **LDPC**: WiFi, televisi√≥n digital

#### 6.3 Bioinform√°tica

Comparaci√≥n de secuencias de ADN:

```
Secuencia 1: ACGTACGT
Secuencia 2: ACGTAGGT
d_H = 2 (diferencias en posiciones 6 y 7)
```

#### 6.4 Procesamiento de Im√°genes

Detecci√≥n de similitud entre im√°genes usando hashing perceptual.

### 7. C√≥digos Gray

Los **c√≥digos Gray** son una aplicaci√≥n especial donde palabras adyacentes tienen d_H = 1.

#### C√≥digo Gray de 3 bits

| Decimal | Binario | Gray |
|---------|---------|------|
| 0 | 000 | 000 |
| 1 | 001 | 001 |
| 2 | 010 | 011 |
| 3 | 011 | 010 |
| 4 | 100 | 110 |
| 5 | 101 | 111 |
| 6 | 110 | 101 |
| 7 | 111 | 100 |

**Propiedad**: Cada transici√≥n cambia exactamente 1 bit.

**Aplicaciones**:

- Encoders rotativos
- Conversi√≥n A/D
- Minimizaci√≥n de errores en transiciones

### 8. Relaci√≥n con C√≥digos de Bloque

Un **c√≥digo de bloque (n, k)** codifica k bits de informaci√≥n en n bits (n > k).

**Tasa de c√≥digo**: R = k/n

**Redundancia**: n - k bits

**Objetivo**: Maximizar R manteniendo $d_{min}$ grande.

#### Ejemplo: Hamming (7,4)

```
n = 7 bits totales
k = 4 bits de datos
Redundancia = 3 bits de paridad
R = 4/7 ‚âà 0.57
$d_{min} = 3$ (corrige 1 error)
```

### 9. Algoritmos de C√°lculo

#### Algoritmo 1: C√°lculo Directo

```python
def hamming_distance(x: str, y: str) -> int:
    """Calcula la distancia de Hamming entre dos cadenas."""
    if len(x) != len(y):
        raise ValueError("Las cadenas deben tener la misma longitud")
    
    return sum(c1 != c2 for c1, c2 in zip(x, y))
```

**Complejidad**: O(n)

#### Algoritmo 2: Usando XOR (para binarias)

```python
def hamming_distance_xor(x: int, y: int) -> int:
    """Distancia de Hamming usando XOR para n√∫meros binarios."""
    xor = x ^ y
    count = 0
    while xor:
        count += xor & 1
        xor >>= 1
    return count
```

**Complejidad**: O(log n)

#### Algoritmo 3: Distancia M√≠nima de un C√≥digo

```python
def min_distance(code: list[str]) -> int:
    """Calcula la distancia m√≠nima de un c√≥digo."""
    n = len(code)
    if n < 2:
        return float('inf')
    
    min_dist = float('inf')
    for i in range(n):
        for j in range(i + 1, n):
            dist = hamming_distance(code[i], code[j])
            min_dist = min(min_dist, dist)
    
    return min_dist
```

**Complejidad**: O(n¬≤ ¬∑ m) donde n = |c√≥digo|, m = longitud de palabra

### 10. Teoremas Avanzados

#### Teorema 4 (Cota de Singleton)

```
|C| ‚â§ |Œ£|^(n-d+1)
```

Para un c√≥digo con distancia m√≠nima d.

#### Teorema 5 (Cota de Plotkin)

Para c√≥digos binarios con d > n/2:

```
|C| ‚â§ 2d / (2d - n)
```

#### Teorema 6 (Cota de Elias-Bassalygo)

Para un c√≥digo binario C de longitud n con distancia m√≠nima d:

```
|C| ‚â§ 2^n / (V(n, ‚åä(d-1)/2‚åã) ¬∑ (1 - R(Œ¥)))
```

donde:

- Œ¥ = d/n es la **distancia relativa**
- R(Œ¥) es una funci√≥n relacionada con la entrop√≠a binaria
- V(n, r) es el volumen de una esfera de Hamming de radio r

**Forma alternativa** usando la funci√≥n de entrop√≠a binaria H(p) = -p log‚ÇÇ(p) - (1-p) log‚ÇÇ(1-p):

Para Œ¥ ‚â§ 1/2 y c√≥digos suficientemente largos:

```
|C| ‚â§ 2^(n(1 - H(Œ¥/2) + o(1)))
```

**Interpretaci√≥n**: Esta cota mejora la cota de Hamming para c√≥digos con distancia relativa moderada, proporcionando un l√≠mite m√°s ajustado sobre el tama√±o m√°ximo del c√≥digo.

**Aplicaci√≥n**: Es especialmente √∫til para analizar c√≥digos asint√≥ticamente buenos y establecer l√≠mites en la teor√≠a de c√≥digos algebraicos.

### 11. Espacio M√©trico de Hamming

El par (Œ£‚Åø, d_H) forma un **espacio m√©trico**:

**Propiedades topol√≥gicas**:

- Espacio discreto (todas las distancias son enteras)
- **No es ultram√©trica**: La distancia de Hamming no satisface la desigualdad triangular fuerte d(x,z) ‚â§ max(d(x,y), d(y,z)). Contraejemplo: x=000, y=111, z=100 da d(x,y)=3 > max(d(x,z), d(y,z))=max(1,2)=2
- Esferas son conjuntos finitos
- No es un espacio normado (no hay noci√≥n de "longitud")

**Embedding**: El espacio de Hamming puede embeberse en ‚Ñù‚Åø con la m√©trica l‚ÇÅ.

### 12. Comparaci√≥n con Otras M√©tricas

| M√©trica | Definici√≥n | Uso |
|---------|------------|-----|
| **Hamming** | N√∫mero de posiciones diferentes | Cadenas de igual longitud |
| **Levenshtein** | M√≠n. operaciones (ins/del/sust) | Cadenas de distinta longitud |
| **Jaccard** | 1 - |A‚à©B|/|A‚à™B| | Conjuntos |
| **Coseno** | 1 - cos(Œ∏) | Vectores (similitud) |
| **Euclidiana** | ‚àöŒ£(xi-yi)¬≤ | Vectores en ‚Ñù‚Åø |

**Nota**: Hamming es un caso especial de Levenshtein cuando solo se permiten sustituciones.

### 13. Demostraci√≥n Formal Computacional

Este proyecto incluye una **demostraci√≥n formal completa** de que la distancia de Hamming es una m√©trica, implementada en el sistema de l√≥gica matem√°tica:

```bash
cd demos
python demo_hamming_metrica.py
```

El demo demuestra formalmente:

1. ‚úì No negatividad e identidad: d(x,y) ‚â• 0 y d(x,y)=0 ‚ü∫ x=y
2. ‚úì Simetr√≠a: d(x,y) = d(y,x)
3. ‚úì Desigualdad triangular: d(x,z) ‚â§ d(x,y) + d(y,z)

Adem√°s valida las propiedades con ejemplos computacionales sobre cadenas binarias.

**Ubicaci√≥n del c√≥digo**: `core/math_logic_system/` contiene el sistema completo de l√≥gica formal que permite construir y verificar demostraciones matem√°ticas rigurosas.

### Resumen

La distancia de Hamming es fundamental para:

- ‚úÖ Teor√≠a de c√≥digos correctores de errores
- ‚úÖ Detecci√≥n y correcci√≥n autom√°tica de errores
- ‚úÖ Dise√±o de sistemas de comunicaci√≥n robustos
- ‚úÖ Bioinform√°tica y comparaci√≥n de secuencias
- ‚úÖ Criptograf√≠a y hashing

Su caracterizaci√≥n como m√©trica formal garantiza propiedades matem√°ticas s√≥lidas que sustentan su uso en aplicaciones cr√≠ticas.

## üîß Funciones Python Asociadas

### M√≥dulos Implementados

#### 1. `core/formal_languages.py`

Funciones b√°sicas para trabajar con lenguajes formales y distancia de Hamming:

```python
from core.formal_languages import hamming_distance, min_distance_of_language

# Calcular distancia entre dos palabras
d = hamming_distance("10110", "10010")  # 1

# Distancia m√≠nima de un c√≥digo
code = ["000", "111", "101", "010"]
d_min = min_distance_of_language(code)  # 2
```

#### 2. `demos/demo_hamming_metrica.py`

Demostraci√≥n formal completa que prueba que la distancia de Hamming es una m√©trica:

```python
from demos.demo_hamming_metrica import (
    create_metric_space_axioms,
    prove_hamming_is_metric,
    validate_with_examples
)

# Crear sistema de axiomas de espacio m√©trico
axioms = create_metric_space_axioms()

# Demostrar formalmente que Hamming es m√©trica
theorems = prove_hamming_is_metric(axioms)

# Validar con ejemplos
validate_with_examples()
```

**Salida**: Prueba formal paso a paso de cada propiedad m√©trica.

#### 3. `core/math_logic_system/`

Sistema completo de l√≥gica matem√°tica para demostraciones formales:

```python
from core.math_logic_system import (
    AxiomSystem, Proof, Theorem, 
    ProofVerifier, ModusPonens
)

# Crear sistema axiom√°tico
system = AxiomSystem("Espacios M√©tricos", "...")

# Construir demostraci√≥n
proof = Proof(goal, "Hamming es m√©trica")
# ... a√±adir pasos ...

# Verificar correcci√≥n
verifier = ProofVerifier(system)
if verifier.verify(proof):
    print("‚úì Demostraci√≥n v√°lida")
```

### Ejemplos de Uso

#### Ejemplo 1: C√°lculo B√°sico

```python
# Distancia entre cadenas binarias
x = "1011010"
y = "1001011"

d = hamming_distance(x, y)
print(f"Distancia: {d}")  # 3

# Posiciones diferentes
differences = [(i, x[i], y[i]) for i in range(len(x)) if x[i] != y[i]]
print(f"Difieren en: {differences}")
# [(2, '1', '0'), (4, '0', '1'), (6, '0', '1')]
```

#### Ejemplo 2: C√≥digo de Hamming (7,4)

```python
# C√≥digo de Hamming con 16 palabras
hamming_7_4 = [
    "0000000", "1101000", "0110100", "1011100",
    "0011010", "1110010", "0101110", "1000110",
    "0001101", "1100101", "0111001", "1010001",
    "0010111", "1111111", "0100011", "1001011"
]

# Calcular distancia m√≠nima
d_min = min_distance_of_language(hamming_7_4)
print(f"Distancia m√≠nima: {d_min}")  # 3

# Capacidad de correcci√≥n
t = (d_min - 1) // 2
print(f"Corrige hasta {t} errores")  # 1
```

#### Ejemplo 3: Esfera de Hamming

```python
from core.formal_languages import hamming_sphere

# Esfera de radio 1 alrededor de "101"
center = "101"
radius = 1
sphere = hamming_sphere(center, radius)

print(f"S_{radius}({center}) = {sphere}")
# {'101', '001', '111', '100'}

print(f"|S_{radius}({center})| = {len(sphere)}")  # 4
# Verificaci√≥n: C(3,0) + C(3,1) = 1 + 3 = 4 ‚úì
```

#### Ejemplo 4: Correcci√≥n de Errores

```python
def decode_nearest_neighbor(received, code):
    """Decodifica usando el vecino m√°s cercano."""
    min_dist = float('inf')
    closest = None
    
    for codeword in code:
        d = hamming_distance(received, codeword)
        if d < min_dist:
            min_dist = d
            closest = codeword
    
    return closest, min_dist

# C√≥digo de repetici√≥n triple
code = ["000", "111"]

# Palabra recibida con error
received = "101"

decoded, dist = decode_nearest_neighbor(received, code)
print(f"Recibido: {received}")
print(f"Decodificado: {decoded}")  # "111"
print(f"Distancia: {dist}")  # 1
```

#### Ejemplo 5: Verificaci√≥n de Propiedades

```python
from demos.demo_hamming_metrica import validate_metric_properties

# Verificar propiedades m√©tricas con ejemplos aleatorios
words = ["0000", "1111", "1010", "0101"]
validate_metric_properties(words)

# Salida:
# ‚úì No negatividad: d(x,y) ‚â• 0
# ‚úì Identidad: d(x,x) = 0
# ‚úì Simetr√≠a: d(x,y) = d(y,x)
# ‚úì Desigualdad triangular: d(x,z) ‚â§ d(x,y) + d(y,z)
```

### Ejecuci√≥n de Demos

```bash
# Demostraci√≥n formal completa
cd demos
python demo_hamming_metrica.py

# Sistema de l√≥gica matem√°tica
python -c "from core.math_logic_system import PeanoArithmetic; p = PeanoArithmetic(); p.show_axioms()"
```

## üìö Recursos Adicionales

### Referencias Acad√©micas

1. **Hamming, R.W.** (1950). "Error detecting and error correcting codes". *Bell System Technical Journal*, 29(2):147-160.
   - Art√≠culo original que introduce la distancia de Hamming

2. **MacWilliams, F.J. & Sloane, N.J.A.** (1977). *The Theory of Error-Correcting Codes*. North-Holland.
   - Tratado completo sobre c√≥digos correctores

3. **Lin, S. & Costello, D.J.** (2004). *Error Control Coding* (2nd ed.). Prentice Hall.
   - Texto moderno sobre teor√≠a de c√≥digos

4. **Roth, R.M.** (2006). *Introduction to Coding Theory*. Cambridge University Press.
   - Introducci√≥n matem√°tica rigurosa

### Recursos en L√≠nea

- [Wikipedia: Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance)
- [Stanford CS259: Information Theory](https://web.stanford.edu/class/cs259/)
- [MIT 6.02: Introduction to EECS II (Coding Theory)](https://ocw.mit.edu/)

### Implementaciones de Referencia

- **SciPy**: `scipy.spatial.distance.hamming`
- **NumPy**: C√°lculo vectorizado
- **Hamming**: Librer√≠a especializada en Python

### Material del Proyecto

- **C√≥digo fuente**: `core/formal_languages.py`, `core/math_logic_system/`
- **Demos**: `demos/demo_hamming_metrica.py`
- **Tests**: `tests/test_formal_languages.py`
- **Documentaci√≥n API**: `core/math_logic_system/README.md`

## ‚úÖ Estado de Desarrollo

- [x] Teor√≠a documentada
  - [x] Definici√≥n formal
  - [x] Demostraci√≥n de propiedades m√©tricas
  - [x] Aplicaciones y ejemplos
  - [x] Teoremas avanzados
  - [x] Algoritmos de c√°lculo
  
- [x] Ejemplos a√±adidos
  - [x] C√°lculo b√°sico
  - [x] C√≥digos correctores (Hamming 7,4)
  - [x] Esferas de Hamming
  - [x] Correcci√≥n de errores
  - [x] C√≥digo Gray
  
- [x] Funciones Python implementadas
  - [x] `hamming_distance()` - C√°lculo b√°sico
  - [x] `min_distance_of_language()` - Distancia m√≠nima
  - [x] `hamming_sphere()` - Esfera de radio r
  - [x] `decode_nearest_neighbor()` - Decodificaci√≥n
  
- [x] Sistema de demostraci√≥n formal
  - [x] Demostraci√≥n de que Hamming es m√©trica
  - [x] Verificador autom√°tico de pruebas
  - [x] Biblioteca de teoremas reutilizables
  
- [x] Tests unitarios creados
  - [x] Tests para `hamming_distance()`
  - [x] Tests para propiedades m√©tricas
  - [x] Tests para c√≥digos correctores
  - [x] Tests de integraci√≥n

### Pr√≥ximas Mejoras

- [ ] Visualizaci√≥n de esferas de Hamming
- [ ] Implementaci√≥n de c√≥digos de Hamming (n,k) generales
- [ ] Algoritmos de decodificaci√≥n eficientes
- [ ] Comparaci√≥n de rendimiento con otras m√©tricas
- [ ] Integraci√≥n con m√≥dulos de c√≥digos especializados (BCD, Gray, etc.)

---

**√öltima actualizaci√≥n**: Enero 2026  
**Estado**: ‚úÖ Documentaci√≥n completa con demostraciones formales
